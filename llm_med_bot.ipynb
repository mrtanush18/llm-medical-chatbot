{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GspF8nVw-dt5"
   },
   "outputs": [],
   "source": [
    "# Step1: Create & Setup hugging face API token in Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z87270vtsVl7",
    "outputId": "542d6a3b-ec39-45a8-de47-923a2fb34972"
   },
   "outputs": [],
   "source": [
    "!pip install unsloth # install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6w5Z3IRI5Ciz",
    "outputId": "cb2ae85d-42ba-4b2a-9a91-49e7424fd6a5"
   },
   "outputs": [],
   "source": [
    "# Step3: Import necessary libraries\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from huggingface_hub import login\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnNkaxSa5EG8"
   },
   "outputs": [],
   "source": [
    "# Step4: Check HF token\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbMzRW7Q-rI1",
    "outputId": "e976ed87-5bc1-45bc-f1fd-2f8e7f235030"
   },
   "outputs": [],
   "source": [
    "# Optional: Check GPU availability\n",
    "# Test if CUDA is available\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9uJ0kyU5EJC",
    "outputId": "56145eb7-85a5-44b4-869b-35e2e4101741"
   },
   "outputs": [],
   "source": [
    "# Step5: Setup pretrained DeepSeek-R1\n",
    "\n",
    "model_name = \"dee/DeepSeek-R1-Distill-Llama-8B\"\n",
    "max_sequence_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_sequence_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywwTJ-EX-wZS"
   },
   "outputs": [],
   "source": [
    "# Step6: Setup system prompt\n",
    "prompt_style = \"\"\"\n",
    "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
    "\n",
    "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
    "\n",
    "### Task:\n",
    "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
    "\n",
    "### Query:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "<think>{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXlU4R1M-x9T",
    "outputId": "ad6af05a-c8b6-4575-f1c1-b2c476b40fe4"
   },
   "outputs": [],
   "source": [
    "# Step7: Run Inference on the model\n",
    "\n",
    "# Define a test question\n",
    "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\n",
    "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
    "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQRw7EOj-x_Y",
    "outputId": "d65ec3eb-3494-4434-ac62-785569444beb"
   },
   "outputs": [],
   "source": [
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1rGOKPF-yB1"
   },
   "outputs": [],
   "source": [
    "# Step8: Setup fine-tuning\n",
    "\n",
    "# Load Dataset\n",
    "medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:500]\", trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGxWOJVn-yD7",
    "outputId": "5ec1aa8a-aa3e-4b19-b617-adb5241ba4c2"
   },
   "outputs": [],
   "source": [
    "medical_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CRtCjhOZ-yGS",
    "outputId": "31a766a4-d948-498a-96dc-c4d1efc40b4e"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training\n",
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHVFpt51862S"
   },
   "outputs": [],
   "source": [
    "### Finetuning\n",
    "# Updated training prompt style to add </think> tag\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
    "Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrKFL1ec87CP"
   },
   "outputs": [],
   "source": [
    "# Prepare the data for fine-tuning\n",
    "\n",
    "def preprocess_input_data(examples):\n",
    "  inputs = examples[\"Question\"]\n",
    "  cots = examples[\"Complex_CoT\"]\n",
    "  outputs = examples[\"Response\"]\n",
    "\n",
    "  texts = []\n",
    "\n",
    "  for input, cot, output in zip(inputs, cots, outputs):\n",
    "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return {\n",
    "      \"texts\" : texts,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cbf6243bb0544c1d8dd29c6acc6728d7",
      "83e9bd571ec44b6892b1278483276897",
      "9637ad61fd9b42458131070fc3519018",
      "c9ed19caaa6f423383748361f2cda94b",
      "c1a21470a10f4b39af495c4aa165b97e",
      "fb11021cf4c34765887612db2120c9b2",
      "2d1f66a743f04471826c66a633ed9784",
      "6ef5dbd8844d40ca8cc23945b2083061",
      "e505ea81431e46cea2b6ab779334e23d",
      "83c68ef8600e4b148ad38bcfd0120a54",
      "a1b91f74fe6849eaad1826204da48abd"
     ]
    },
    "id": "OTsrjryT_RLF",
    "outputId": "af7bd5bd-8bc1-417b-9f3b-17a9a9ac1086"
   },
   "outputs": [],
   "source": [
    "finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "crLgEm6h9BNB",
    "outputId": "1a9ab8c3-3c9b-4007-c22e-0c5ca836a5d3"
   },
   "outputs": [],
   "source": [
    "finetune_dataset[\"texts\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p32ium159BPF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyLZqq8R5ELJ"
   },
   "outputs": [],
   "source": [
    "# Step9: Setup/Apply LoRA finetuning to the model\n",
    "\n",
    "model_lora = FastLanguageModel.get_peft_model(\n",
    "    model = model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3047,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_1xufC2Yt3K"
   },
   "outputs": [],
   "source": [
    "# Add this before creating the trainer\n",
    "if hasattr(model, '_unwrapped_old_generate'):\n",
    "    del model._unwrapped_old_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c79f8144383e432598d517cff083630a",
      "4b435b94193a4ff4afbb2fa0e88a832d",
      "10d5588981754c3485d69be91be1c7ce",
      "1c1a6b64dd4f49d29e6104189174d267",
      "18672a0155f448afb8e845edffd2869b",
      "0cbf4e0df05642dabfd349076e92122c",
      "1b6e284092dd48e481450de489fa06e4",
      "3508fe1ba271429da5077113e1a87f1f",
      "24c998142c1a4bcda89c09576def3d3a",
      "13ac29a1b95f4757b62c111cc431d13a",
      "1543a848f2934e4ea32c00fa16c0b27c",
      "5868574017d0412c9e8bc9059e7f703b",
      "0636fe7103b54a46b5aa32d95c70acbe",
      "d1051f69f42a49b89f32bb133cf305cc",
      "c2386147a57f41e29a1aa8e331fea799",
      "fede4221781b4d059cb6b4588dde1634",
      "73d62b8629c54b50bef676bf69965ea5",
      "d63f3d55e04249eabcd19801c39a2298",
      "3b02b1805e5d4f3eaa32b91158089ea8",
      "f4c0d01f2d3a49118e03c6c590d02cd4",
      "298aa8fbe78045a485e5b5dc1957697b",
      "e903505dcffe4f2db93612e6976efd43"
     ]
    },
    "id": "NenTCAT45ENx",
    "outputId": "1dbc7905-7479-47ec-988f-1ce64f51d85e"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model_lora,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = finetune_dataset,\n",
    "    dataset_text_field = \"texts\",\n",
    "    max_seq_length = max_sequence_length,\n",
    "    dataset_num_proc = 1,\n",
    "\n",
    "    # Define training args\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ST9J4i8PZjYy",
    "outputId": "8e6ca232-e1c0-4cc3-ad83-d84ebd9a4f52"
   },
   "outputs": [],
   "source": [
    "# Setup WANDB\n",
    "from google.colab import userdata\n",
    "wnb_token = userdata.get(\"WANDB_API_TOKEN\")\n",
    "# Login to WnB\n",
    "wandb.login(key=wnb_token) # import wandb\n",
    "run = wandb.init(\n",
    "    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "irR021n75EP4",
    "outputId": "ba01d20d-5146-4c87-a85a-faa4fba2b626"
   },
   "outputs": [],
   "source": [
    "# Start the fine-tuning process\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "XwTqmIfz_uNd",
    "outputId": "b95e654d-2fe3-4a7f-bfa3-8cbc67b160bc"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ofzlf5UlfP3o",
    "outputId": "d081b7e3-b7ba-42a4-ca6a-5377aff82ade"
   },
   "outputs": [],
   "source": [
    "# Step10: Testing after fine-tuning\n",
    "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
    "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
    "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model_lora)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model_lora.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jhG3-7Lf-ej",
    "outputId": "a0d5d16d-06ef-4ded-e727-ee80c6332c71"
   },
   "outputs": [],
   "source": [
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDlOpre6gTln",
    "outputId": "a2ec25c6-bc11-4d13-aaea-a1823302ee95"
   },
   "outputs": [],
   "source": [
    "question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,\n",
    "              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,\n",
    "              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.\n",
    "              What is the most likely predisposing factor for this patient's condition?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model_lora)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model_lora.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response[0].split(\"### Answer:\")[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
